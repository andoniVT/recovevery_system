La computacion paralela es una tecnica de programacion en la que muchas instrucciones se ejecutan simultaneamente.1 Se basa en el principio de que los problemas grandes se pueden dividir en partes mas pequenas que pueden resolverse de forma paralela (no es lo mismo que concurrente). Existen varios tipos de computacion paralela: paralelismo a nivel de bit, paralelismo a nivel de instruccion, paralelismo de datos y paralelismo de tareas. Durante muchos anos, la computacion paralela se ha aplicado en la computacion de altas prestaciones, pero el interes en ella ha aumentado en los ultimos anos debido a las restricciones fisicas que impiden el escalado en frecuencia. La computacion paralela se ha convertido en el paradigma dominante en la arquitectura de computadores, principalmente en los procesadores multinucleo.2 Sin embargo, recientemente, el consumo de energia de los ordenadores paralelos se ha convertido en una preocupacion.3
Los ordenadores paralelos se pueden clasificar segun el nivel de paralelismo que admite su hardware: los ordenadores multinucleo y multiproceso tienen varios elementos de procesamiento en una sola maquina, mientras que los clusters, los MPP y los grids emplean varios ordenadores para trabajar en la misma tarea.
Los programas de ordenador paralelos son mas dificiles de escribir que los secuenciales4 porque el paralelismo introduce nuevos tipos de errores de software, siendo las condiciones de carrera los mas comunes. La comunicacion y la sincronizacion entre las diferentes subtareas son tipicamente las grandes barreras para conseguir un buen rendimiento de los programas paralelos. El incremento de velocidad que consigue un programa como resultado de la paralelizacion viene dado por la ley de Amdahl.
